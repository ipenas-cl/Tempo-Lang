// â•”â•â•â•â•â•â•¦â•â•â•â•â•â•¦â•â•â•â•â•â•—
// â•‘ ğŸ›¡ï¸  â•‘ âš–ï¸  â•‘ âš¡  â•‘
// â•‘  C  â•‘  E  â•‘  G  â•‘
// â•šâ•â•â•â•â•â•©â•â•â•â•â•â•©â•â•â•â•â•â•
// â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
// â•‘ wcet [Tâˆ] bound â•‘
// â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//
// Author: Ignacio PeÃ±a SepÃºlveda
// Date: June 25, 2025

// Memory management module

// Page sizes
const PAGE_SIZE: u64 = 4096;
const LARGE_PAGE_SIZE: u64 = 2 * 1024 * 1024;  // 2MB
const HUGE_PAGE_SIZE: u64 = 1024 * 1024 * 1024;  // 1GB

// Page table entry flags
const PTE_PRESENT: u64 = 1 << 0;
const PTE_WRITABLE: u64 = 1 << 1;
const PTE_USER: u64 = 1 << 2;
const PTE_WRITE_THROUGH: u64 = 1 << 3;
const PTE_CACHE_DISABLE: u64 = 1 << 4;
const PTE_ACCESSED: u64 = 1 << 5;
const PTE_DIRTY: u64 = 1 << 6;
const PTE_LARGE_PAGE: u64 = 1 << 7;
const PTE_GLOBAL: u64 = 1 << 8;
const PTE_NO_EXECUTE: u64 = 1 << 63;

// Virtual address breakdown for 4-level paging
fn pml4_index(addr: u64) -> u64 {
    return (addr >> 39) & 0x1FF;
}

fn pdpt_index(addr: u64) -> u64 {
    return (addr >> 30) & 0x1FF;
}

fn pd_index(addr: u64) -> u64 {
    return (addr >> 21) & 0x1FF;
}

fn pt_index(addr: u64) -> u64 {
    return (addr >> 12) & 0x1FF;
}

fn page_offset(addr: u64) -> u64 {
    return addr & 0xFFF;
}

// Physical memory allocator (simple bump allocator for bootloader)
struct PhysicalMemoryAllocator {
    next_free: u64,
    end: u64,
}

var pmm: PhysicalMemoryAllocator;

// Initialize physical memory manager
fn pmm_init(start: u64, size: u64) {
    pmm.next_free = (start + PAGE_SIZE - 1) & ~(PAGE_SIZE - 1);  // Align to page
    pmm.end = start + size;
}

// Allocate physical page
fn pmm_alloc_page() -> u64 {
    if pmm.next_free >= pmm.end {
        return 0;  // Out of memory
    }
    
    let page = pmm.next_free;
    pmm.next_free += PAGE_SIZE;
    
    // Clear the page
    let ptr = page as *u64;
    for i in 0..(PAGE_SIZE / 8) {
        ptr[i] = 0;
    }
    
    return page;
}

// Allocate contiguous physical pages
fn pmm_alloc_pages(count: u64) -> u64 {
    let size = count * PAGE_SIZE;
    
    if pmm.next_free + size > pmm.end {
        return 0;  // Out of memory
    }
    
    let pages = pmm.next_free;
    pmm.next_free += size;
    
    // Clear the pages
    let ptr = pages as *u64;
    for i in 0..(size / 8) {
        ptr[i] = 0;
    }
    
    return pages;
}

// Map a virtual address to a physical address
fn map_page(pml4: *u64, vaddr: u64, paddr: u64, flags: u64) {
    let pml4_idx = pml4_index(vaddr);
    let pdpt_idx = pdpt_index(vaddr);
    let pd_idx = pd_index(vaddr);
    let pt_idx = pt_index(vaddr);
    
    // Get or create PDPT
    let mut pdpt: *u64;
    if (pml4[pml4_idx] & PTE_PRESENT) == 0 {
        pdpt = pmm_alloc_page() as *u64;
        pml4[pml4_idx] = (pdpt as u64) | PTE_PRESENT | PTE_WRITABLE;
    } else {
        pdpt = (pml4[pml4_idx] & ~0xFFF) as *u64;
    }
    
    // Get or create PD
    let mut pd: *u64;
    if (pdpt[pdpt_idx] & PTE_PRESENT) == 0 {
        pd = pmm_alloc_page() as *u64;
        pdpt[pdpt_idx] = (pd as u64) | PTE_PRESENT | PTE_WRITABLE;
    } else {
        pd = (pdpt[pdpt_idx] & ~0xFFF) as *u64;
    }
    
    // Get or create PT
    let mut pt: *u64;
    if (pd[pd_idx] & PTE_PRESENT) == 0 {
        pt = pmm_alloc_page() as *u64;
        pd[pd_idx] = (pt as u64) | PTE_PRESENT | PTE_WRITABLE;
    } else {
        pt = (pd[pd_idx] & ~0xFFF) as *u64;
    }
    
    // Map the page
    pt[pt_idx] = paddr | flags | PTE_PRESENT;
}

// Map a range of pages
fn map_range(pml4: *u64, vstart: u64, pstart: u64, size: u64, flags: u64) {
    let pages = (size + PAGE_SIZE - 1) / PAGE_SIZE;
    
    for i in 0..pages {
        map_page(pml4, vstart + i * PAGE_SIZE, pstart + i * PAGE_SIZE, flags);
    }
}

// Identity map a range (virtual = physical)
fn identity_map_range(pml4: *u64, start: u64, size: u64, flags: u64) {
    map_range(pml4, start, start, size, flags);
}