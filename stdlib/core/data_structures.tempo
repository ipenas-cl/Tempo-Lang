// â•”â•â•â•â•â•â•¦â•â•â•â•â•â•¦â•â•â•â•â•â•—
// â•‘ ðŸ›¡ï¸  â•‘ âš–ï¸  â•‘ âš¡  â•‘
// â•‘  C  â•‘  E  â•‘  G  â•‘
// â•šâ•â•â•â•â•â•©â•â•â•â•â•â•©â•â•â•â•â•â•
// â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
// â•‘ wcet [Tâˆž] bound â•‘
// â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//
// Author: Ignacio PeÃ±a SepÃºlveda
// Date: June 25, 2025

// Optimized data structures for Redis Killer
// Zero-allocation implementations with cache-friendly layouts

import "std/sync"
import "std/atomic"

// SIMD-optimized string comparison
@inline
func fast_string_eq(a: &[u8], b: &[u8]) -> bool {
    if a.len() != b.len() {
        return false
    }
    
    let len = a.len()
    let mut i = 0
    
    // Process 8 bytes at a time using u64 comparison
    while i + 8 <= len {
        let a_chunk = unsafe { *(a.as_ptr().add(i) as *const u64) }
        let b_chunk = unsafe { *(b.as_ptr().add(i) as *const u64) }
        if a_chunk != b_chunk {
            return false
        }
        i += 8
    }
    
    // Handle remaining bytes
    while i < len {
        if a[i] != b[i] {
            return false
        }
        i += 1
    }
    
    true
}

// Lock-free memory pool for zero-allocation operations
const POOL_CHUNK_SIZE = 4096
const MAX_POOL_CHUNKS = 16384

struct MemoryPool {
    chunks: [MAX_POOL_CHUNKS][POOL_CHUNK_SIZE]u8
    chunk_states: [MAX_POOL_CHUNKS]atomic.AtomicU32  // 0=free, 1=allocated
    free_list: atomic.AtomicU64  // Packed index and generation
}

impl MemoryPool {
    func new() -> MemoryPool {
        MemoryPool{}
    }
    
    func alloc(&self, size: usize) -> Option<&mut [u8]> {
        if size > POOL_CHUNK_SIZE {
            return None
        }
        
        // Fast path: try to allocate from free list
        loop {
            let packed = self.free_list.load(atomic.Ordering::Acquire)
            let idx = (packed & 0xFFFFFFFF) as u32
            
            if idx >= MAX_POOL_CHUNKS {
                // Free list empty, scan for free chunk
                for i in 0..MAX_POOL_CHUNKS {
                    if self.chunk_states[i].compare_exchange_weak(
                        0, 1, 
                        atomic.Ordering::AcqRel,
                        atomic.Ordering::Relaxed
                    ).is_ok() {
                        return Some(&mut self.chunks[i][..size])
                    }
                }
                return None  // Pool exhausted
            }
            
            // Try to claim this chunk
            let generation = (packed >> 32) as u32
            let new_packed = ((generation + 1) as u64) << 32  // Increment generation, clear index
            
            if self.free_list.compare_exchange_weak(
                packed, new_packed,
                atomic.Ordering::AcqRel,
                atomic.Ordering::Relaxed
            ).is_ok() {
                // Successfully claimed
                self.chunk_states[idx].store(1, atomic.Ordering::Release)
                return Some(&mut self.chunks[idx][..size])
            }
        }
    }
    
    func free(&self, ptr: &[u8]) {
        // Calculate chunk index from pointer
        let base_ptr = self.chunks.as_ptr() as usize
        let ptr_addr = ptr.as_ptr() as usize
        let offset = ptr_addr - base_ptr
        let idx = offset / POOL_CHUNK_SIZE
        
        // Mark as free
        self.chunk_states[idx].store(0, atomic.Ordering::Release)
        
        // Add to free list
        loop {
            let old_packed = self.free_list.load(atomic.Ordering::Acquire)
            let generation = (old_packed >> 32) as u32
            let new_packed = ((generation as u64) << 32) | (idx as u64)
            
            if self.free_list.compare_exchange_weak(
                old_packed, new_packed,
                atomic.Ordering::AcqRel,
                atomic.Ordering::Relaxed
            ).is_ok() {
                break
            }
        }
    }
}

// Cache-aligned hash table bucket for better performance
#[repr(align(64))]  // Cache line aligned
struct HashBucket {
    entries: [8]BucketEntry  // 8 entries per bucket for better cache utilization
    lock: sync.SpinLock      // Fine-grained locking per bucket
}

struct BucketEntry {
    key_hash: u64
    key_ptr: *const u8
    key_len: u16
    value_ptr: *const u8
    value_len: u32
    expire_time: i64
}

// Optimized sorted set implementation using skip list with memory pooling
struct SortedSet {
    skip_list: PooledSkipList
    member_index: HashMap<string, f64>  // Member -> score mapping
}

struct PooledSkipList {
    pool: MemoryPool
    head: *SkipNode
    tail: *SkipNode
    level: u8
    size: u32
}

struct SkipNode {
    score: f64
    member: string
    backward: *SkipNode
    level: []*SkipNode  // Forward pointers
}

impl SortedSet {
    func new() -> SortedSet {
        SortedSet{
            skip_list: PooledSkipList::new(),
            member_index: HashMap::new(),
        }
    }
    
    func zadd(&mut self, member: string, score: f64) -> bool {
        if let Some(old_score) = self.member_index.get(&member) {
            if *old_score == score {
                return false  // No change
            }
            // Remove old entry
            self.skip_list.remove(*old_score, &member)
        }
        
        // Insert new entry
        self.skip_list.insert(score, member.clone())
        self.member_index.insert(member, score)
        true
    }
    
    func zrange(&self, start: i32, stop: i32) -> Vec<(string, f64)> {
        let mut result = Vec::new()
        let mut count = 0
        let mut node = self.skip_list.head.level[0]
        
        // Convert negative indices
        let len = self.skip_list.size as i32
        let start = if start < 0 { len + start } else { start }
        let stop = if stop < 0 { len + stop } else { stop }
        
        // Skip to start position
        while node != self.skip_list.tail && count < start {
            node = node.level[0]
            count += 1
        }
        
        // Collect range
        while node != self.skip_list.tail && count <= stop {
            result.push((node.member.clone(), node.score))
            node = node.level[0]
            count += 1
        }
        
        result
    }
}

// Optimized list implementation with deque for O(1) push/pop
struct RedisList {
    deque: CircularBuffer<string>
}

struct CircularBuffer<T> {
    buffer: Vec<T>
    head: usize
    tail: usize
    size: usize
    capacity: usize
}

impl<T> CircularBuffer<T> {
    func new(capacity: usize) -> CircularBuffer<T> {
        CircularBuffer{
            buffer: Vec::with_capacity(capacity),
            head: 0,
            tail: 0,
            size: 0,
            capacity: capacity,
        }
    }
    
    func push_front(&mut self, value: T) {
        if self.size == self.capacity {
            self.grow()
        }
        
        self.head = (self.head + self.capacity - 1) % self.capacity
        self.buffer[self.head] = value
        self.size += 1
    }
    
    func push_back(&mut self, value: T) {
        if self.size == self.capacity {
            self.grow()
        }
        
        self.buffer[self.tail] = value
        self.tail = (self.tail + 1) % self.capacity
        self.size += 1
    }
    
    func pop_front(&mut self) -> Option<T> {
        if self.size == 0 {
            return None
        }
        
        let value = self.buffer[self.head].take()
        self.head = (self.head + 1) % self.capacity
        self.size -= 1
        Some(value)
    }
    
    func pop_back(&mut self) -> Option<T> {
        if self.size == 0 {
            return None
        }
        
        self.tail = (self.tail + self.capacity - 1) % self.capacity
        self.size -= 1
        Some(self.buffer[self.tail].take())
    }
    
    func grow(&mut self) {
        let new_capacity = self.capacity * 2
        let mut new_buffer = Vec::with_capacity(new_capacity)
        
        // Copy elements in order
        let mut i = self.head
        for _ in 0..self.size {
            new_buffer.push(self.buffer[i].take())
            i = (i + 1) % self.capacity
        }
        
        self.buffer = new_buffer
        self.capacity = new_capacity
        self.head = 0
        self.tail = self.size
    }
}

// HyperLogLog for cardinality estimation
const HLL_REGISTERS = 16384  // 2^14 registers for 0.81% error
const HLL_BITS = 6           // 6 bits per register

struct HyperLogLog {
    registers: [HLL_REGISTERS]u8
    modified: bool
}

impl HyperLogLog {
    func new() -> HyperLogLog {
        HyperLogLog{
            modified: false,
        }
    }
    
    func add(&mut self, data: &[u8]) {
        let hash = murmur3_hash(data)
        let idx = (hash & ((HLL_REGISTERS - 1) as u64)) as usize
        let rho = leading_zeros(hash >> 14) + 1  // Position of first 1 bit
        
        if rho > self.registers[idx] {
            self.registers[idx] = rho as u8
            self.modified = true
        }
    }
    
    func count(&self) -> u64 {
        let mut raw_estimate = 0.0
        let mut zeros = 0
        
        for reg in self.registers {
            if reg == 0 {
                zeros += 1
            }
            raw_estimate += 1.0 / (1u64 << reg) as f64
        }
        
        raw_estimate = HLL_ALPHA * (HLL_REGISTERS * HLL_REGISTERS) as f64 / raw_estimate
        
        // Apply bias correction
        if raw_estimate <= 2.5 * HLL_REGISTERS as f64 {
            if zeros != 0 {
                raw_estimate = HLL_REGISTERS as f64 * (HLL_REGISTERS as f64 / zeros as f64).ln()
            }
        } else if raw_estimate <= (1.0 / 30.0) * (1u64 << 32) as f64 {
            // No correction
        } else {
            raw_estimate = -(1u64 << 32) as f64 * (1.0 - raw_estimate / (1u64 << 32) as f64).ln()
        }
        
        raw_estimate as u64
    }
}

const HLL_ALPHA: f64 = 0.7213 / (1.0 + 1.079 / HLL_REGISTERS as f64)

// MurmurHash3 for HyperLogLog
func murmur3_hash(data: &[u8]) -> u64 {
    let mut h1 = 0u64
    let mut h2 = 0u64
    
    let c1 = 0x87c37b91114253d5u64
    let c2 = 0x4cf5ad432745937fu64
    
    let blocks = data.len() / 16
    
    for i in 0..blocks {
        let k1 = unsafe { *((data.as_ptr().add(i * 16)) as *const u64) }
        let k2 = unsafe { *((data.as_ptr().add(i * 16 + 8)) as *const u64) }
        
        let k1 = k1.wrapping_mul(c1).rotate_left(31).wrapping_mul(c2)
        h1 ^= k1
        h1 = h1.rotate_left(27).wrapping_add(h2).wrapping_mul(5).wrapping_add(0x52dce729)
        
        let k2 = k2.wrapping_mul(c2).rotate_left(33).wrapping_mul(c1)
        h2 ^= k2
        h2 = h2.rotate_left(31).wrapping_add(h1).wrapping_mul(5).wrapping_add(0x38495ab5)
    }
    
    // Handle tail
    let tail = &data[blocks * 16..]
    if tail.len() > 0 {
        let mut k1 = 0u64
        let mut k2 = 0u64
        
        for i in 0..tail.len().min(8) {
            k1 |= (tail[i] as u64) << (i * 8)
        }
        for i in 8..tail.len() {
            k2 |= (tail[i] as u64) << ((i - 8) * 8)
        }
        
        if tail.len() > 0 {
            k1 = k1.wrapping_mul(c1).rotate_left(31).wrapping_mul(c2)
            h1 ^= k1
        }
        if tail.len() > 8 {
            k2 = k2.wrapping_mul(c2).rotate_left(33).wrapping_mul(c1)
            h2 ^= k2
        }
    }
    
    // Finalization
    h1 ^= data.len() as u64
    h2 ^= data.len() as u64
    
    h1 = h1.wrapping_add(h2)
    h2 = h2.wrapping_add(h1)
    
    h1 = fmix64(h1)
    h2 = fmix64(h2)
    
    h1.wrapping_add(h2)
}

func fmix64(mut k: u64) -> u64 {
    k ^= k >> 33
    k = k.wrapping_mul(0xff51afd7ed558ccdu64)
    k ^= k >> 33
    k = k.wrapping_mul(0xc4ceb9fe1a85ec53u64)
    k ^= k >> 33
    k
}

func leading_zeros(x: u64) -> u8 {
    if x == 0 {
        return 64
    }
    
    let mut n = 0
    let mut y = x
    
    if y & 0xFFFFFFFF00000000 == 0 { n += 32; y <<= 32 }
    if y & 0xFFFF000000000000 == 0 { n += 16; y <<= 16 }
    if y & 0xFF00000000000000 == 0 { n += 8; y <<= 8 }
    if y & 0xF000000000000000 == 0 { n += 4; y <<= 4 }
    if y & 0xC000000000000000 == 0 { n += 2; y <<= 2 }
    if y & 0x8000000000000000 == 0 { n += 1 }
    
    n
}