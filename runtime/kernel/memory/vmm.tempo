// â•”â•â•â•â•â•â•¦â•â•â•â•â•â•¦â•â•â•â•â•â•—
// â•‘ ğŸ›¡ï¸  â•‘ âš–ï¸  â•‘ âš¡  â•‘
// â•‘  C  â•‘  E  â•‘  G  â•‘
// â•šâ•â•â•â•â•â•©â•â•â•â•â•â•©â•â•â•â•â•â•
// â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
// â•‘ wcet [Tâˆ] bound â•‘
// â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//
// Author: Ignacio PeÃ±a SepÃºlveda
// Date: June 25, 2025

// AtomicOS Virtual Memory Manager
// Implements paging, virtual address spaces, and memory protection

module kernel.memory.vmm

import kernel.memory.pmm
import kernel.process.process

// Virtual memory manager state
struct VirtualMemoryManager {
    kernel_page_directory: *PageDirectory
    current_page_directory: *PageDirectory
    
    // Virtual address space layout
    kernel_start: usize
    kernel_end: usize
    user_start: usize
    user_end: usize
    
    // TLB management
    tlb_shootdowns: u64
    
    // Statistics
    page_faults: u64
    mappings: u64
    unmappings: u64
    
    lock: SpinLock
}

// Page directory (PML4 for x86-64)
struct PageDirectory {
    entries: [512]PageTableEntry
    physical_addr: usize
    ref_count: u32
}

// Page table
struct PageTable {
    entries: [512]PageTableEntry
}

// Page table entry
struct PageTableEntry {
    present: bool
    writable: bool
    user: bool
    write_through: bool
    cache_disable: bool
    accessed: bool
    dirty: bool
    huge_page: bool
    global: bool
    no_execute: bool
    physical_addr: usize
}

// Virtual memory flags
const VMM_FLAG_PRESENT = 1 << 0
const VMM_FLAG_WRITABLE = 1 << 1
const VMM_FLAG_USER = 1 << 2
const VMM_FLAG_WRITE_THROUGH = 1 << 3
const VMM_FLAG_CACHE_DISABLE = 1 << 4
const VMM_FLAG_HUGE_PAGE = 1 << 7
const VMM_FLAG_GLOBAL = 1 << 8
const VMM_FLAG_NO_EXECUTE = 1 << 63
const VMM_FLAG_KERNEL = 1 << 10  // Custom flag

// Address space layout
const KERNEL_SPACE_START = 0xFFFF800000000000
const KERNEL_SPACE_END = 0xFFFFFFFFFFFFFFFF
const USER_SPACE_START = 0x0000000000000000
const USER_SPACE_END = 0x00007FFFFFFFFFFF

const PAGE_SIZE = 4096
const HUGE_PAGE_SIZE = 2097152  // 2MB
const ENTRIES_PER_TABLE = 512

// Initialize virtual memory manager
@export
fn init(pmm: *PhysicalMemoryManager) -> *VirtualMemoryManager {
    let vmm = alloc(VirtualMemoryManager)
    
    vmm.kernel_start = KERNEL_SPACE_START
    vmm.kernel_end = KERNEL_SPACE_END
    vmm.user_start = USER_SPACE_START
    vmm.user_end = USER_SPACE_END
    
    vmm.lock = SpinLock{}
    
    // Create kernel page directory
    vmm.kernel_page_directory = create_page_directory()
    vmm.current_page_directory = vmm.kernel_page_directory
    
    // Set up initial kernel mappings
    setup_kernel_mappings(vmm)
    
    // Enable paging
    enable_paging(vmm.kernel_page_directory.physical_addr)
    
    return vmm
}

// Create new page directory
@export
fn create_page_directory() -> *PageDirectory {
    // Allocate page for directory
    let phys_addr = pmm.alloc_page()
    if phys_addr == 0 {
        return null
    }
    
    let pd = phys_to_virt(phys_addr) as *PageDirectory
    pd.physical_addr = phys_addr
    pd.ref_count = 1
    
    // Clear all entries
    for i in 0..ENTRIES_PER_TABLE {
        clear_pte(&pd.entries[i])
    }
    
    return pd
}

// Destroy page directory
@export
fn destroy_page_directory(pd: *PageDirectory) {
    if pd == null || pd == get_vmm().kernel_page_directory {
        return
    }
    
    pd.ref_count--
    if pd.ref_count > 0 {
        return
    }
    
    // Free all page tables
    for i in 0..ENTRIES_PER_TABLE {
        if pd.entries[i].present && !is_kernel_space(i << 39) {
            let pt_phys = pd.entries[i].physical_addr
            free_page_table(pt_phys)
        }
    }
    
    // Free page directory
    pmm.free_page(pd.physical_addr)
}

// Map virtual address to physical address
@export
fn map_page(pd: *PageDirectory, virt_addr: usize, phys_addr: usize, flags: u64) -> bool {
    let vmm = get_vmm()
    
    vmm.lock.acquire()
    defer vmm.lock.release()
    
    // Get page table indices
    let pml4_idx = (virt_addr >> 39) & 0x1FF
    let pdpt_idx = (virt_addr >> 30) & 0x1FF
    let pdt_idx = (virt_addr >> 21) & 0x1FF
    let pt_idx = (virt_addr >> 12) & 0x1FF
    
    // Get or create page tables
    let pdpt = get_or_create_table(pd, pml4_idx, flags)
    if pdpt == null {
        return false
    }
    
    let pdt = get_or_create_table(pdpt, pdpt_idx, flags)
    if pdt == null {
        return false
    }
    
    let pt = get_or_create_table(pdt, pdt_idx, flags)
    if pt == null {
        return false
    }
    
    // Map the page
    let pte = &pt.entries[pt_idx]
    pte.physical_addr = phys_addr
    pte.present = (flags & VMM_FLAG_PRESENT) != 0
    pte.writable = (flags & VMM_FLAG_WRITABLE) != 0
    pte.user = (flags & VMM_FLAG_USER) != 0
    pte.write_through = (flags & VMM_FLAG_WRITE_THROUGH) != 0
    pte.cache_disable = (flags & VMM_FLAG_CACHE_DISABLE) != 0
    pte.no_execute = (flags & VMM_FLAG_NO_EXECUTE) != 0
    
    // Flush TLB entry
    flush_tlb_single(virt_addr)
    
    vmm.mappings++
    
    return true
}

// Map range of pages
@export
fn map_range(pd: *PageDirectory, virt_start: usize, phys_start: usize, size: usize, flags: u64) -> bool {
    let page_count = (size + PAGE_SIZE - 1) / PAGE_SIZE
    
    for i in 0..page_count {
        let virt_addr = virt_start + i * PAGE_SIZE
        let phys_addr = phys_start + i * PAGE_SIZE
        
        if !map_page(pd, virt_addr, phys_addr, flags) {
            // Unmap what we've mapped so far
            for j in 0..i {
                unmap_page(pd, virt_start + j * PAGE_SIZE)
            }
            return false
        }
    }
    
    return true
}

// Unmap virtual address
@export
fn unmap_page(pd: *PageDirectory, virt_addr: usize) {
    let vmm = get_vmm()
    
    vmm.lock.acquire()
    defer vmm.lock.release()
    
    // Get page table indices
    let pml4_idx = (virt_addr >> 39) & 0x1FF
    let pdpt_idx = (virt_addr >> 30) & 0x1FF
    let pdt_idx = (virt_addr >> 21) & 0x1FF
    let pt_idx = (virt_addr >> 12) & 0x1FF
    
    // Navigate to page table
    let pml4e = &pd.entries[pml4_idx]
    if !pml4e.present {
        return
    }
    
    let pdpt = phys_to_virt(pml4e.physical_addr) as *PageTable
    let pdpte = &pdpt.entries[pdpt_idx]
    if !pdpte.present {
        return
    }
    
    let pdt = phys_to_virt(pdpte.physical_addr) as *PageTable
    let pdte = &pdt.entries[pdt_idx]
    if !pdte.present {
        return
    }
    
    let pt = phys_to_virt(pdte.physical_addr) as *PageTable
    let pte = &pt.entries[pt_idx]
    
    // Clear the entry
    clear_pte(pte)
    
    // Flush TLB
    flush_tlb_single(virt_addr)
    
    vmm.unmappings++
}

// Get physical address for virtual address
@export
fn virt_to_phys(pd: *PageDirectory, virt_addr: usize) -> usize {
    // Get page table indices
    let pml4_idx = (virt_addr >> 39) & 0x1FF
    let pdpt_idx = (virt_addr >> 30) & 0x1FF
    let pdt_idx = (virt_addr >> 21) & 0x1FF
    let pt_idx = (virt_addr >> 12) & 0x1FF
    let offset = virt_addr & 0xFFF
    
    // Navigate page tables
    let pml4e = &pd.entries[pml4_idx]
    if !pml4e.present {
        return 0
    }
    
    let pdpt = phys_to_virt(pml4e.physical_addr) as *PageTable
    let pdpte = &pdpt.entries[pdpt_idx]
    if !pdpte.present {
        return 0
    }
    
    // Check for 1GB huge page
    if pdpte.huge_page {
        return pdpte.physical_addr + (virt_addr & 0x3FFFFFFF)
    }
    
    let pdt = phys_to_virt(pdpte.physical_addr) as *PageTable
    let pdte = &pdt.entries[pdt_idx]
    if !pdte.present {
        return 0
    }
    
    // Check for 2MB huge page
    if pdte.huge_page {
        return pdte.physical_addr + (virt_addr & 0x1FFFFF)
    }
    
    let pt = phys_to_virt(pdte.physical_addr) as *PageTable
    let pte = &pt.entries[pt_idx]
    if !pte.present {
        return 0
    }
    
    return pte.physical_addr + offset
}

// Switch page directory
@export
fn switch_page_directory(pd: *PageDirectory) {
    let vmm = get_vmm()
    
    if pd == vmm.current_page_directory {
        return
    }
    
    vmm.current_page_directory = pd
    load_page_directory(pd.physical_addr)
}

// Map kernel space into page directory
@export
fn map_kernel_space(pd: *PageDirectory) {
    // Copy kernel mappings from kernel page directory
    let kernel_pd = get_vmm().kernel_page_directory
    
    // Copy upper half entries (kernel space)
    for i in 256..512 {
        pd.entries[i] = kernel_pd.entries[i]
    }
}

// Copy address space (for fork)
@export
fn copy_address_space(src_pd: *PageDirectory, dst_pd: *PageDirectory) -> bool {
    // Copy user space mappings
    for pml4_idx in 0..256 {
        if !src_pd.entries[pml4_idx].present {
            continue
        }
        
        // TODO: Implement copy-on-write
        if !copy_page_table_hierarchy(src_pd, dst_pd, pml4_idx) {
            return false
        }
    }
    
    // Map kernel space
    map_kernel_space(dst_pd)
    
    return true
}

// Handle page fault
@export
fn handle_page_fault(fault_addr: usize, error_code: u32) {
    let vmm = get_vmm()
    vmm.page_faults++
    
    let present = (error_code & 0x1) != 0
    let write = (error_code & 0x2) != 0
    let user = (error_code & 0x4) != 0
    let reserved = (error_code & 0x8) != 0
    let inst_fetch = (error_code & 0x10) != 0
    
    // Get current process
    let proc = process.get_current_process()
    
    // Check if it's a valid fault we can handle
    if reserved {
        // Reserved bit set - corruption
        kernel.kernel_panic("Page fault: reserved bit set")
    }
    
    if !present {
        // Page not present - check if it's a valid allocation
        if is_valid_user_fault(proc, fault_addr) {
            // Allocate and map page
            allocate_user_page(proc, fault_addr)
            return
        }
    }
    
    // Unhandled page fault
    kprint("Page fault at %p (error: %x)\n", fault_addr, error_code)
    kprint("  Present: %d, Write: %d, User: %d\n", present, write, user)
    
    if user {
        // Kill the process
        process.exit(-1)
    } else {
        // Kernel page fault
        kernel.kernel_panic("Kernel page fault")
    }
}

// Helper functions

fn setup_kernel_mappings(vmm: *VirtualMemoryManager) {
    // Identity map first 1GB for early boot structures
    map_range(
        vmm.kernel_page_directory,
        0,
        0,
        0x40000000,  // 1GB
        VMM_FLAG_PRESENT | VMM_FLAG_WRITABLE | VMM_FLAG_KERNEL
    )
    
    // Map kernel at higher half
    let kernel_phys = kernel.kernel_state.boot_info.kernel_start
    let kernel_size = kernel.kernel_state.boot_info.kernel_end - kernel_phys
    
    map_range(
        vmm.kernel_page_directory,
        KERNEL_SPACE_START,
        kernel_phys,
        kernel_size,
        VMM_FLAG_PRESENT | VMM_FLAG_WRITABLE | VMM_FLAG_KERNEL | VMM_FLAG_GLOBAL
    )
}

fn get_or_create_table(parent: *PageTable, index: usize, flags: u64) -> *PageTable {
    let entry = &parent.entries[index]
    
    if entry.present {
        return phys_to_virt(entry.physical_addr) as *PageTable
    }
    
    // Allocate new table
    let table_phys = pmm.alloc_page()
    if table_phys == 0 {
        return null
    }
    
    // Clear new table
    let table = phys_to_virt(table_phys) as *PageTable
    for i in 0..ENTRIES_PER_TABLE {
        clear_pte(&table.entries[i])
    }
    
    // Set up parent entry
    entry.physical_addr = table_phys
    entry.present = true
    entry.writable = true
    entry.user = (flags & VMM_FLAG_USER) != 0
    
    return table
}

fn clear_pte(pte: *PageTableEntry) {
    pte.present = false
    pte.writable = false
    pte.user = false
    pte.write_through = false
    pte.cache_disable = false
    pte.accessed = false
    pte.dirty = false
    pte.huge_page = false
    pte.global = false
    pte.no_execute = false
    pte.physical_addr = 0
}

fn is_kernel_space(addr: usize) -> bool {
    return addr >= KERNEL_SPACE_START
}

fn is_valid_user_fault(proc: *Process, addr: usize) -> bool {
    // Check if within heap range
    if addr >= proc.heap_start && addr < proc.heap_end {
        return true
    }
    
    // Check if within stack range
    if addr >= USER_STACK_TOP - USER_STACK_SIZE && addr < USER_STACK_TOP {
        return true
    }
    
    return false
}

fn allocate_user_page(proc: *Process, fault_addr: usize) {
    let page_addr = fault_addr & ~(PAGE_SIZE - 1)
    
    // Allocate physical page
    let phys_page = pmm.alloc_page()
    if phys_page == 0 {
        process.exit(-1)  // Out of memory
        return
    }
    
    // Map page
    let flags = VMM_FLAG_PRESENT | VMM_FLAG_WRITABLE | VMM_FLAG_USER
    if !map_page(proc.page_directory, page_addr, phys_page, flags) {
        pmm.free_page(phys_page)
        process.exit(-1)
    }
    
    // Clear page
    let page_virt = phys_to_virt(phys_page)
    memset(page_virt, 0, PAGE_SIZE)
}

fn free_page_table(phys_addr: usize) {
    let table = phys_to_virt(phys_addr) as *PageTable
    
    // Free all pages mapped by this table
    for i in 0..ENTRIES_PER_TABLE {
        if table.entries[i].present {
            // Note: We don't free the physical pages here
            // as they might be shared (COW) or belong to files
        }
    }
    
    pmm.free_page(phys_addr)
}

fn copy_page_table_hierarchy(src_pd: *PageDirectory, dst_pd: *PageDirectory, pml4_idx: usize) -> bool {
    // TODO: Implement full hierarchy copy with COW
    return true
}

// Physical to virtual address conversion
fn phys_to_virt(phys_addr: usize) -> usize {
    // Assuming identity mapping in kernel space
    return phys_addr + KERNEL_SPACE_START
}

// Get VMM instance
fn get_vmm() -> *VirtualMemoryManager {
    return kernel.kernel_state.vmm
}

// Architecture-specific functions
@extern fn enable_paging(page_directory: usize)
@extern fn load_page_directory(page_directory: usize)
@extern fn flush_tlb_single(addr: usize)
@extern fn flush_tlb_all()

// Constants
const USER_STACK_SIZE = 1048576  // 1MB
const USER_STACK_TOP = 0x7FFFFFFFFFFF