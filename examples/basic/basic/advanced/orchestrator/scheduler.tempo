// ╔═════╦═════╦═════╗
// ║ 🛡️  ║ ⚖️  ║ ⚡  ║
// ║  C  ║  E  ║  G  ║
// ╚═════╩═════╩═════╝
// ╔═════════════════╗
// ║ wcet [T∞] bound ║
// ╚═════════════════╝
//
// Author: Ignacio Peña Sepúlveda
// Date: June 25, 2025

// Real-time Aware Pod Scheduling
// Deterministic scheduling with priority and deadline support

namespace scheduler {
    using orchestrator::{Node, Pod, ResourceRequirements, NodeStatus, PodStatus};
    
    // Scheduling constraints
    const MAX_SCHEDULING_ITERATIONS = 100;
    const SCHEDULING_QUANTUM = 1ms;
    const MAX_PREEMPTION_ATTEMPTS = 5;
    const SCHEDULING_CACHE_SIZE = 1000;
    
    // Scheduling classes for real-time guarantees
    enum SchedulingClass {
        RealTimeHard,      // Hard real-time with strict deadlines
        RealTimeSoft,      // Soft real-time with flexible deadlines
        HighPriority,      // High priority batch
        NormalPriority,    // Default priority
        LowPriority,       // Best effort
        Preemptible        // Can be preempted by any higher class
    }
    
    // Deterministic scheduler with real-time support
    struct DeterministicScheduler {
        algorithm: SchedulingAlgorithm;
        node_scores: array<NodeScore, 1000>;  // Pre-allocated scoring array
        scheduling_queue: PriorityQueue<SchedulingRequest>;
        preemption_enabled: bool;
        overcommit_ratio: f32;
        affinity_weight: f32;
        anti_affinity_weight: f32;
        node_selector_weight: f32;
        resource_weight: f32;
        latency_weight: f32;
        cache: SchedulingCache;
        metrics: SchedulingMetrics;
    }
    
    struct SchedulingRequest {
        pod_id: u64;
        scheduling_class: SchedulingClass;
        priority: u8;
        deadline: timestamp?;
        submit_time: timestamp;
        attempts: u8;
    }
    
    struct NodeScore {
        node_id: u32;
        score: i32;
        feasible: bool;
        preemption_candidates: array<u64, 10>;  // Pod IDs that could be preempted
        preemption_count: u8;
    }
    
    struct SchedulingCache {
        node_capacities: hashmap<u32, NodeCapacity>;
        pod_affinities: hashmap<u64, AffinityRules>;
        last_update: timestamp;
    }
    
    struct NodeCapacity {
        total_cpu: f32;
        available_cpu: f32;
        total_memory_mb: u32;
        available_memory_mb: u32;
        pod_count: u8;
        running_scheduling_classes: u8;  // Bitmask of scheduling classes
    }
    
    struct AffinityRules {
        node_affinity: NodeAffinity?;
        pod_affinity: array<PodAffinityRule, 10>;
        pod_anti_affinity: array<PodAffinityRule, 10>;
        affinity_count: u8;
        anti_affinity_count: u8;
    }
    
    struct NodeAffinity {
        required_labels: array<LabelSelector, 10>;
        preferred_labels: array<WeightedLabelSelector, 10>;
        required_count: u8;
        preferred_count: u8;
    }
    
    struct PodAffinityRule {
        label_selector: LabelSelector;
        topology_key: string<64>;  // e.g., "node", "zone", "region"
        namespaces: array<string<64>, 10>;
        namespace_count: u8;
    }
    
    struct LabelSelector {
        key: string<64>;
        operator: SelectorOperator;
        values: array<string<64>, 10>;
        value_count: u8;
    }
    
    struct WeightedLabelSelector {
        weight: u8;  // 1-100
        selector: LabelSelector;
    }
    
    enum SelectorOperator {
        In,
        NotIn,
        Exists,
        DoesNotExist,
        Equals,
        NotEquals
    }
    
    struct SchedulingMetrics {
        total_scheduled: u64;
        total_failed: u64;
        total_preempted: u64;
        average_scheduling_time_us: u32;
        cache_hits: u64;
        cache_misses: u64;
    }
    
    // Main scheduling operation
    operation find_node_for_pod(scheduler: &mut DeterministicScheduler,
                                nodes: &[Node],
                                pod: &Pod) -> Result<u32, SchedulingError> 
                                within SCHEDULING_QUANTUM {
        let start_time = current_time();
        
        // Create scheduling request
        let request = SchedulingRequest {
            pod_id: pod.id,
            scheduling_class: pod.scheduling_class,
            priority: pod.priority,
            deadline: extract_deadline(pod),
            submit_time: start_time,
            attempts: 0
        };
        
        // Update cache if needed
        if should_update_cache(&scheduler.cache, start_time) {
            update_scheduling_cache(&mut scheduler.cache, nodes);
        }
        
        // Filter feasible nodes
        let mut feasible_count = 0;
        for (i, node) in nodes.iter().enumerate() {
            scheduler.node_scores[i] = evaluate_node_feasibility(
                scheduler,
                node,
                pod,
                &scheduler.cache
            );
            if scheduler.node_scores[i].feasible {
                feasible_count += 1;
            }
        }
        
        if feasible_count == 0 {
            // No feasible nodes - check if preemption can help
            if scheduler.preemption_enabled && 
               can_preempt_for_pod(pod.scheduling_class) {
                return try_scheduling_with_preemption(scheduler, nodes, pod);
            }
            return Err(SchedulingError::NoFeasibleNodes);
        }
        
        // Score nodes based on scheduling algorithm
        score_nodes(scheduler, nodes, pod, feasible_count);
        
        // Select best node
        let best_node_id = select_best_node(&scheduler.node_scores[0..nodes.len()]);
        
        // Update metrics
        let scheduling_time = (current_time() - start_time).as_micros() as u32;
        update_scheduling_metrics(&mut scheduler.metrics, true, scheduling_time);
        
        Ok(best_node_id)
    }
    
    // Node feasibility check
    fn evaluate_node_feasibility(scheduler: &DeterministicScheduler,
                                node: &Node,
                                pod: &Pod,
                                cache: &SchedulingCache) -> NodeScore {
        let mut score = NodeScore {
            node_id: node.id,
            score: 0,
            feasible: true,
            preemption_candidates: array::new(),
            preemption_count: 0
        };
        
        // Check node status
        if node.status != NodeStatus::Ready {
            score.feasible = false;
            return score;
        }
        
        // Check resource availability with overcommit
        let effective_cpu = node.available_cpu_cores * scheduler.overcommit_ratio;
        let effective_memory = (node.available_memory_mb as f32 * scheduler.overcommit_ratio) as u32;
        
        if pod.resource_requirements.cpu_cores > effective_cpu ||
           pod.resource_requirements.memory_mb > effective_memory {
            score.feasible = false;
            
            // Find preemption candidates if enabled
            if scheduler.preemption_enabled {
                find_preemption_candidates(
                    node,
                    pod,
                    &mut score.preemption_candidates,
                    &mut score.preemption_count
                );
            }
        }
        
        // Check node selector constraints
        if !check_node_selector(node, pod) {
            score.feasible = false;
            return score;
        }
        
        // Check taints and tolerations
        if !check_taints_and_tolerations(node, pod) {
            score.feasible = false;
            return score;
        }
        
        // Check pod count limit
        if node.pod_count >= MAX_PODS_PER_NODE {
            score.feasible = false;
        }
        
        score
    }
    
    // Node scoring based on algorithm
    fn score_nodes(scheduler: &mut DeterministicScheduler,
                   nodes: &[Node],
                   pod: &Pod,
                   feasible_count: usize) {
        match scheduler.algorithm {
            SchedulingAlgorithm::BinPacking => {
                score_bin_packing(scheduler, nodes, pod);
            },
            SchedulingAlgorithm::Spreading => {
                score_spreading(scheduler, nodes, pod);
            },
            SchedulingAlgorithm::RealTimePriority => {
                score_real_time_priority(scheduler, nodes, pod);
            }
        }
    }
    
    // Bin packing scoring - minimize number of nodes
    fn score_bin_packing(scheduler: &mut DeterministicScheduler,
                        nodes: &[Node],
                        pod: &Pod) {
        for i in 0..nodes.len() {
            if !scheduler.node_scores[i].feasible {
                continue;
            }
            
            let node = &nodes[i];
            let mut score = 0i32;
            
            // Higher score for nodes with less available resources (more packed)
            let cpu_utilization = 1.0 - (node.available_cpu_cores / node.total_cpu_cores as f32);
            let memory_utilization = 1.0 - (node.available_memory_mb as f32 / node.total_memory_mb as f32);
            
            score += (cpu_utilization * 50.0) as i32;
            score += (memory_utilization * 50.0) as i32;
            
            // Apply affinity scoring
            score += calculate_affinity_score(scheduler, node, pod);
            
            scheduler.node_scores[i].score = score;
        }
    }
    
    // Spreading scoring - maximize distribution
    fn score_spreading(scheduler: &mut DeterministicScheduler,
                      nodes: &[Node],
                      pod: &Pod) {
        for i in 0..nodes.len() {
            if !scheduler.node_scores[i].feasible {
                continue;
            }
            
            let node = &nodes[i];
            let mut score = 0i32;
            
            // Higher score for nodes with more available resources (less loaded)
            let cpu_availability = node.available_cpu_cores / node.total_cpu_cores as f32;
            let memory_availability = node.available_memory_mb as f32 / node.total_memory_mb as f32;
            
            score += (cpu_availability * 50.0) as i32;
            score += (memory_availability * 50.0) as i32;
            
            // Bonus for nodes with fewer pods
            score += ((MAX_PODS_PER_NODE - node.pod_count) * 2) as i32;
            
            // Apply affinity scoring
            score += calculate_affinity_score(scheduler, node, pod);
            
            scheduler.node_scores[i].score = score;
        }
    }
    
    // Real-time priority scoring
    fn score_real_time_priority(scheduler: &mut DeterministicScheduler,
                               nodes: &[Node],
                               pod: &Pod) {
        for i in 0..nodes.len() {
            if !scheduler.node_scores[i].feasible {
                continue;
            }
            
            let node = &nodes[i];
            let mut score = 0i32;
            
            // Consider scheduling class priority
            match pod.scheduling_class {
                SchedulingClass::RealTimeHard => {
                    // Prefer dedicated nodes or nodes with few RT pods
                    score += 100;
                    score -= (count_rt_pods_on_node(node) * 20) as i32;
                },
                SchedulingClass::RealTimeSoft => {
                    score += 80;
                    score -= (count_rt_pods_on_node(node) * 10) as i32;
                },
                SchedulingClass::HighPriority => {
                    score += 60;
                },
                SchedulingClass::NormalPriority => {
                    score += 40;
                },
                SchedulingClass::LowPriority => {
                    score += 20;
                },
                SchedulingClass::Preemptible => {
                    score += 10;
                }
            }
            
            // Factor in deadline if present
            if let Some(deadline) = pod.deadline {
                let time_to_deadline = deadline - current_time();
                if time_to_deadline < 10s {
                    score += 50;  // Urgent scheduling
                }
            }
            
            // Consider node latency/performance characteristics
            score += calculate_latency_score(node, pod);
            
            // Apply affinity scoring
            score += calculate_affinity_score(scheduler, node, pod);
            
            scheduler.node_scores[i].score = score;
        }
    }
    
    // Preemption logic
    fn try_scheduling_with_preemption(scheduler: &mut DeterministicScheduler,
                                     nodes: &[Node],
                                     pod: &Pod) -> Result<u32, SchedulingError> {
        let mut best_preemption_node = None;
        let mut min_preemptions = MAX_PREEMPTION_ATTEMPTS + 1;
        
        for i in 0..nodes.len() {
            let score = &scheduler.node_scores[i];
            if score.preemption_count > 0 && score.preemption_count < min_preemptions {
                // Verify preemption would free enough resources
                if can_fit_after_preemption(nodes[i], pod, &score.preemption_candidates) {
                    best_preemption_node = Some(nodes[i].id);
                    min_preemptions = score.preemption_count;
                }
            }
        }
        
        if let Some(node_id) = best_preemption_node {
            // Record preemption decision
            scheduler.metrics.total_preempted += min_preemptions as u64;
            Ok(node_id)
        } else {
            Err(SchedulingError::PreemptionFailed)
        }
    }
    
    // Helper functions
    fn calculate_affinity_score(scheduler: &DeterministicScheduler,
                               node: &Node,
                               pod: &Pod) -> i32 {
        let mut score = 0i32;
        
        // Check if we have affinity rules cached
        if let Some(affinity) = scheduler.cache.pod_affinities.get(&pod.id) {
            // Node affinity scoring
            if let Some(node_affinity) = &affinity.node_affinity {
                score += evaluate_node_affinity(node, node_affinity);
            }
            
            // Pod affinity scoring
            for i in 0..affinity.affinity_count {
                let rule = &affinity.pod_affinity[i as usize];
                if matches_pod_affinity(node, rule) {
                    score += (scheduler.affinity_weight * 10.0) as i32;
                }
            }
            
            // Pod anti-affinity scoring
            for i in 0..affinity.anti_affinity_count {
                let rule = &affinity.pod_anti_affinity[i as usize];
                if matches_pod_affinity(node, rule) {
                    score -= (scheduler.anti_affinity_weight * 10.0) as i32;
                }
            }
        }
        
        score
    }
    
    fn calculate_latency_score(node: &Node, pod: &Pod) -> i32 {
        // Estimate network latency based on node location
        // This is simplified - in practice would use actual latency measurements
        match pod.scheduling_class {
            SchedulingClass::RealTimeHard | SchedulingClass::RealTimeSoft => {
                // Prefer nodes with lower latency for real-time pods
                20 - estimate_node_latency(node)
            },
            _ => 0
        }
    }
    
    fn select_best_node(scores: &[NodeScore]) -> u32 {
        let mut best_score = i32::MIN;
        let mut best_node_id = 0;
        
        for score in scores {
            if score.feasible && score.score > best_score {
                best_score = score.score;
                best_node_id = score.node_id;
            }
        }
        
        best_node_id
    }
    
    fn can_preempt_for_pod(scheduling_class: SchedulingClass) -> bool {
        match scheduling_class {
            SchedulingClass::RealTimeHard => true,
            SchedulingClass::RealTimeSoft => true,
            SchedulingClass::HighPriority => true,
            SchedulingClass::NormalPriority => false,
            SchedulingClass::LowPriority => false,
            SchedulingClass::Preemptible => false
        }
    }
    
    fn find_preemption_candidates(node: &Node,
                                 pod: &Pod,
                                 candidates: &mut array<u64, 10>,
                                 count: &mut u8) {
        // Find lower priority pods that could be preempted
        // This is simplified - actual implementation would check pod details
        let mut freed_cpu = 0.0f32;
        let mut freed_memory = 0u32;
        
        for i in 0..node.pod_count {
            let pod_ref = &node.pods[i as usize];
            // Check if this pod can be preempted
            if can_preempt_pod(pod_ref.pod_id, pod.scheduling_class) {
                candidates[*count as usize] = pod_ref.pod_id;
                *count += 1;
                
                // Track freed resources
                freed_cpu += pod_ref.resource_usage.cpu_cores_used;
                freed_memory += pod_ref.resource_usage.memory_mb_used;
                
                // Check if we've freed enough resources
                if freed_cpu >= pod.resource_requirements.cpu_cores &&
                   freed_memory >= pod.resource_requirements.memory_mb {
                    break;
                }
                
                if *count >= 10 {
                    break;  // Limit preemption candidates
                }
            }
        }
    }
    
    fn update_scheduling_metrics(metrics: &mut SchedulingMetrics,
                               success: bool,
                               time_us: u32) {
        if success {
            metrics.total_scheduled += 1;
        } else {
            metrics.total_failed += 1;
        }
        
        // Update average scheduling time
        let total = metrics.total_scheduled + metrics.total_failed;
        metrics.average_scheduling_time_us = 
            ((metrics.average_scheduling_time_us as u64 * (total - 1) + time_us as u64) / total) as u32;
    }
    
    enum SchedulingError {
        NoFeasibleNodes,
        PreemptionFailed,
        DeadlineExceeded,
        ResourcesUnavailable,
        AffinityConflict
    }
    
    // Pod specification extension for scheduling
    struct PodSpec {
        name: string<128>;
        namespace: string<64>;
        containers: Vec<ContainerSpec>;
        scheduling_class: SchedulingClass;
        priority: u8;
        deadline: timestamp?;
        node_selector: hashmap<string, string>?;
        affinity: AffinityRules?;
        tolerations: Vec<Toleration>?;
        network_policy: NetworkPolicy;
        volumes: Vec<VolumeMount>;
    }
    
    struct Toleration {
        key: string<64>;
        operator: TolerationOperator;
        value: string<64>?;
        effect: TaintEffect;
        toleration_seconds: u32?;
    }
    
    enum TolerationOperator {
        Equal,
        Exists
    }
    
    enum TaintEffect {
        NoSchedule,
        PreferNoSchedule,
        NoExecute
    }
}