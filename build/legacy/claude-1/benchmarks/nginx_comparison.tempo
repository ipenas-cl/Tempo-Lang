// ╔═════╦═════╦═════╗
// ║ 🛡️  ║ ⚖️  ║ ⚡  ║
// ║  C  ║  E  ║  G  ║
// ╚═════╩═════╩═════╝
// ╔═════════════════╗
// ║ wcet [T∞] bound ║
// ╚═════════════════╝
//
// Author: Ignacio Peña Sepúlveda
// Date: June 25, 2025

// Nginx Comparison Benchmark Suite
// Comprehensive performance testing against Nginx

import "std/net"
import "std/time"
import "std/thread"
import "std/atomic"
import "std/io"
import "std/sys"
import "std/math"
import "std/process"

// Benchmark configuration
struct BenchmarkConfig {
    nginx_port: u16,
    destroyer_port: u16,
    duration: u64,           // seconds
    connections: u32,        // concurrent connections
    threads: u32,            // worker threads
    requests_per_conn: u32,  // requests per connection
    keepalive: bool,
    pipelining: u32,         // number of pipelined requests
    request_size: usize,     // POST body size
    file_sizes: Vec<usize>,  // static file sizes to test
}

// Benchmark results
struct BenchmarkResults {
    server: String,
    total_requests: u64,
    successful_requests: u64,
    failed_requests: u64,
    total_bytes: u64,
    duration_ms: u64,
    
    // Throughput metrics
    requests_per_sec: f64,
    bytes_per_sec: f64,
    
    // Latency metrics (microseconds)
    latency_min: u64,
    latency_max: u64,
    latency_mean: f64,
    latency_p50: u64,
    latency_p90: u64,
    latency_p95: u64,
    latency_p99: u64,
    latency_p999: u64,
    latency_stdev: f64,
    
    // Connection metrics
    connections_per_sec: f64,
    avg_requests_per_conn: f64,
    
    // CPU and memory usage
    cpu_usage: f64,
    memory_usage: u64,
}

// Worker statistics
struct WorkerStats {
    requests: atomic.AtomicU64,
    successful: atomic.AtomicU64,
    failed: atomic.AtomicU64,
    bytes: atomic.AtomicU64,
    latencies: Vec<u64>,
}

// Main benchmark runner
struct NginxComparison {
    config: BenchmarkConfig,
    nginx_results: BenchmarkResults,
    destroyer_results: BenchmarkResults,
}

impl NginxComparison {
    fn new(config: BenchmarkConfig) -> Self {
        NginxComparison {
            config,
            nginx_results: BenchmarkResults::default(),
            destroyer_results: BenchmarkResults::default(),
        }
    }
    
    fn run(&mut self) -> Result<(), Error> {
        println!("=== Nginx Destroyer vs Nginx Performance Comparison ===\n");
        
        // Warm up both servers
        println!("Warming up servers...");
        self.warmup(self.config.nginx_port)?;
        self.warmup(self.config.destroyer_port)?;
        
        // Run benchmarks
        println!("\nRunning benchmarks...");
        
        // Test 1: Basic GET requests
        println!("\n1. Basic GET Request Performance");
        self.benchmark_get_requests()?;
        
        // Test 2: Keep-alive performance
        println!("\n2. Keep-Alive Performance");
        self.benchmark_keepalive()?;
        
        // Test 3: Static file serving
        println!("\n3. Static File Serving");
        self.benchmark_static_files()?;
        
        // Test 4: POST request handling
        println!("\n4. POST Request Performance");
        self.benchmark_post_requests()?;
        
        // Test 5: Concurrent connection handling
        println!("\n5. Concurrent Connection Scaling");
        self.benchmark_connection_scaling()?;
        
        // Test 6: HTTP pipelining
        println!("\n6. HTTP Pipelining Performance");
        self.benchmark_pipelining()?;
        
        // Test 7: Load balancing
        println!("\n7. Load Balancing Performance");
        self.benchmark_load_balancing()?;
        
        // Generate report
        self.generate_report();
        
        Ok(())
    }
    
    fn warmup(&self, port: u16) -> Result<(), Error> {
        let client = HttpClient::new()?;
        
        // Send a few requests to warm up the server
        for _ in 0..100 {
            let mut conn = client.connect("127.0.0.1", port)?;
            conn.send_request("GET", "/", None)?;
            conn.read_response()?;
        }
        
        Ok(())
    }
    
    fn benchmark_get_requests(&mut self) -> Result<(), Error> {
        let config = BenchmarkConfig {
            connections: 100,
            duration: 30,
            keepalive: false,
            requests_per_conn: 1000,
            ..self.config.clone()
        };
        
        // Test Nginx
        println!("  Testing Nginx...");
        self.nginx_results = self.run_benchmark("nginx", self.config.nginx_port, &config)?;
        
        // Test Nginx Destroyer
        println!("  Testing Nginx Destroyer...");
        self.destroyer_results = self.run_benchmark("destroyer", self.config.destroyer_port, &config)?;
        
        // Compare results
        self.print_comparison("GET Requests");
        
        Ok(())
    }
    
    fn benchmark_keepalive(&mut self) -> Result<(), Error> {
        let config = BenchmarkConfig {
            connections: 50,
            duration: 30,
            keepalive: true,
            requests_per_conn: 10000,
            ..self.config.clone()
        };
        
        println!("  Testing Nginx...");
        self.nginx_results = self.run_benchmark("nginx", self.config.nginx_port, &config)?;
        
        println!("  Testing Nginx Destroyer...");
        self.destroyer_results = self.run_benchmark("destroyer", self.config.destroyer_port, &config)?;
        
        self.print_comparison("Keep-Alive");
        
        Ok(())
    }
    
    fn benchmark_static_files(&mut self) -> Result<(), Error> {
        for &size in &[1024, 10240, 102400, 1048576] {  // 1KB, 10KB, 100KB, 1MB
            println!(f"\n  File size: {} bytes", size);
            
            let config = BenchmarkConfig {
                connections: 100,
                duration: 20,
                keepalive: true,
                file_sizes: vec![size],
                ..self.config.clone()
            };
            
            println!("    Testing Nginx...");
            self.nginx_results = self.run_static_benchmark("nginx", self.config.nginx_port, &config)?;
            
            println!("    Testing Nginx Destroyer...");
            self.destroyer_results = self.run_static_benchmark("destroyer", self.config.destroyer_port, &config)?;
            
            self.print_comparison(&format!("Static Files ({})", format_bytes(size)));
        }
        
        Ok(())
    }
    
    fn benchmark_post_requests(&mut self) -> Result<(), Error> {
        let config = BenchmarkConfig {
            connections: 100,
            duration: 30,
            keepalive: true,
            request_size: 4096,  // 4KB POST body
            ..self.config.clone()
        };
        
        println!("  Testing Nginx...");
        self.nginx_results = self.run_post_benchmark("nginx", self.config.nginx_port, &config)?;
        
        println!("  Testing Nginx Destroyer...");
        self.destroyer_results = self.run_post_benchmark("destroyer", self.config.destroyer_port, &config)?;
        
        self.print_comparison("POST Requests");
        
        Ok(())
    }
    
    fn benchmark_connection_scaling(&mut self) -> Result<(), Error> {
        for &conns in &[100, 500, 1000, 5000, 10000] {
            println!(f"\n  Concurrent connections: {}", conns);
            
            let config = BenchmarkConfig {
                connections: conns,
                duration: 30,
                keepalive: true,
                requests_per_conn: 100,
                ..self.config.clone()
            };
            
            println!("    Testing Nginx...");
            self.nginx_results = self.run_benchmark("nginx", self.config.nginx_port, &config)?;
            
            println!("    Testing Nginx Destroyer...");
            self.destroyer_results = self.run_benchmark("destroyer", self.config.destroyer_port, &config)?;
            
            self.print_comparison(&format!("{}k Connections", conns / 1000));
        }
        
        Ok(())
    }
    
    fn benchmark_pipelining(&mut self) -> Result<(), Error> {
        let config = BenchmarkConfig {
            connections: 100,
            duration: 30,
            keepalive: true,
            pipelining: 10,  // 10 pipelined requests
            ..self.config.clone()
        };
        
        println!("  Testing Nginx...");
        self.nginx_results = self.run_pipeline_benchmark("nginx", self.config.nginx_port, &config)?;
        
        println!("  Testing Nginx Destroyer...");
        self.destroyer_results = self.run_pipeline_benchmark("destroyer", self.config.destroyer_port, &config)?;
        
        self.print_comparison("HTTP Pipelining");
        
        Ok(())
    }
    
    fn benchmark_load_balancing(&mut self) -> Result<(), Error> {
        // This would test load balancing features
        // For now, we'll simulate with multiple backend connections
        
        let config = BenchmarkConfig {
            connections: 200,
            duration: 30,
            keepalive: true,
            ..self.config.clone()
        };
        
        println!("  Testing Nginx...");
        self.nginx_results = self.run_benchmark("nginx", self.config.nginx_port, &config)?;
        
        println!("  Testing Nginx Destroyer...");
        self.destroyer_results = self.run_benchmark("destroyer", self.config.destroyer_port, &config)?;
        
        self.print_comparison("Load Balancing");
        
        Ok(())
    }
    
    fn run_benchmark(&self, name: &str, port: u16, config: &BenchmarkConfig) -> Result<BenchmarkResults, Error> {
        let start_time = time::now();
        let stop_time = start_time + time::Duration::from_secs(config.duration);
        
        // Create worker statistics
        let workers = (0..config.threads).map(|_| {
            Arc::new(WorkerStats {
                requests: atomic.AtomicU64::new(0),
                successful: atomic.AtomicU64::new(0),
                failed: atomic.AtomicU64::new(0),
                bytes: atomic.AtomicU64::new(0),
                latencies: Vec::with_capacity(100000),
            })
        }).collect::<Vec<_>>();
        
        // Monitor system resources
        let cpu_monitor = self.start_cpu_monitor(name)?;
        
        // Spawn worker threads
        let handles = workers.iter().enumerate().map(|(id, stats)| {
            let stats = stats.clone();
            let stop = stop_time;
            let cfg = config.clone();
            thread::spawn(move || {
                self.worker_thread(id, port, &cfg, stats, stop);
            })
        }).collect::<Vec<_>>();
        
        // Wait for completion
        for handle in handles {
            handle.join()?;
        }
        
        let duration_ms = (time::now() - start_time).as_millis() as u64;
        
        // Stop CPU monitor
        let (cpu_usage, memory_usage) = self.stop_cpu_monitor(cpu_monitor)?;
        
        // Aggregate results
        let mut all_latencies = Vec::new();
        let mut total_requests = 0u64;
        let mut successful_requests = 0u64;
        let mut failed_requests = 0u64;
        let mut total_bytes = 0u64;
        
        for worker in &workers {
            total_requests += worker.requests.load(atomic.Ordering.Relaxed);
            successful_requests += worker.successful.load(atomic.Ordering.Relaxed);
            failed_requests += worker.failed.load(atomic.Ordering.Relaxed);
            total_bytes += worker.bytes.load(atomic.Ordering.Relaxed);
            all_latencies.extend(&worker.latencies);
        }
        
        // Calculate latency percentiles
        all_latencies.sort_unstable();
        let latency_stats = self.calculate_latency_stats(&all_latencies);
        
        Ok(BenchmarkResults {
            server: name.to_string(),
            total_requests,
            successful_requests,
            failed_requests,
            total_bytes,
            duration_ms,
            requests_per_sec: (total_requests as f64 * 1000.0) / duration_ms as f64,
            bytes_per_sec: (total_bytes as f64 * 1000.0) / duration_ms as f64,
            connections_per_sec: (config.connections as f64 * 1000.0) / duration_ms as f64,
            avg_requests_per_conn: total_requests as f64 / config.connections as f64,
            cpu_usage,
            memory_usage,
            ..latency_stats
        })
    }
    
    fn worker_thread(&self, id: usize, port: u16, config: &BenchmarkConfig, 
                    stats: Arc<WorkerStats>, stop_time: time::Time) {
        let connections_per_worker = config.connections / config.threads;
        let mut clients = Vec::with_capacity(connections_per_worker as usize);
        
        // Create connections
        for _ in 0..connections_per_worker {
            if let Ok(client) = HttpClient::connect("127.0.0.1", port) {
                clients.push(client);
            }
        }
        
        // Run requests
        let mut request_count = 0;
        while time::now() < stop_time {
            for client in &mut clients {
                let start = time::now();
                
                match client.send_request("GET", "/", None) {
                    Ok(_) => {
                        match client.read_response() {
                            Ok(response) => {
                                let latency = (time::now() - start).as_micros() as u64;
                                stats.latencies.push(latency);
                                stats.successful.fetch_add(1, atomic.Ordering.Relaxed);
                                stats.bytes.fetch_add(response.len() as u64, atomic.Ordering.Relaxed);
                            }
                            Err(_) => {
                                stats.failed.fetch_add(1, atomic.Ordering.Relaxed);
                            }
                        }
                    }
                    Err(_) => {
                        stats.failed.fetch_add(1, atomic.Ordering.Relaxed);
                        // Reconnect
                        if let Ok(new_client) = HttpClient::connect("127.0.0.1", port) {
                            *client = new_client;
                        }
                    }
                }
                
                stats.requests.fetch_add(1, atomic.Ordering.Relaxed);
                request_count += 1;
                
                if !config.keepalive || request_count % config.requests_per_conn == 0 {
                    // Close and reconnect
                    if let Ok(new_client) = HttpClient::connect("127.0.0.1", port) {
                        *client = new_client;
                    }
                }
            }
        }
    }
    
    fn calculate_latency_stats(&self, latencies: &[u64]) -> BenchmarkResults {
        if latencies.is_empty() {
            return BenchmarkResults::default();
        }
        
        let len = latencies.len();
        let min = latencies[0];
        let max = latencies[len - 1];
        
        let sum: u64 = latencies.iter().sum();
        let mean = sum as f64 / len as f64;
        
        let variance: f64 = latencies.iter()
            .map(|&x| {
                let diff = x as f64 - mean;
                diff * diff
            })
            .sum::<f64>() / len as f64;
        
        let stdev = variance.sqrt();
        
        BenchmarkResults {
            latency_min: min,
            latency_max: max,
            latency_mean: mean,
            latency_p50: latencies[len * 50 / 100],
            latency_p90: latencies[len * 90 / 100],
            latency_p95: latencies[len * 95 / 100],
            latency_p99: latencies[len * 99 / 100],
            latency_p999: latencies[len * 999 / 1000],
            latency_stdev: stdev,
            ..BenchmarkResults::default()
        }
    }
    
    fn start_cpu_monitor(&self, process_name: &str) -> Result<process::Child, Error> {
        // Start monitoring CPU and memory usage
        let cmd = format!("pidstat -h -r -u -p $(pgrep {}) 1", process_name);
        process::Command::new("sh")
            .arg("-c")
            .arg(&cmd)
            .stdout(process::Stdio::piped())
            .spawn()
    }
    
    fn stop_cpu_monitor(&self, mut monitor: process::Child) -> Result<(f64, u64), Error> {
        monitor.kill()?;
        let output = monitor.wait_with_output()?;
        
        // Parse pidstat output
        let output_str = String::from_utf8_lossy(&output.stdout);
        let lines: Vec<&str> = output_str.lines().collect();
        
        let mut cpu_sum = 0.0;
        let mut mem_sum = 0u64;
        let mut count = 0;
        
        for line in lines {
            let parts: Vec<&str> = line.split_whitespace().collect();
            if parts.len() >= 8 {
                if let Ok(cpu) = parts[7].parse::<f64>() {
                    cpu_sum += cpu;
                    count += 1;
                }
                if let Ok(mem) = parts[6].parse::<u64>() {
                    mem_sum += mem;
                }
            }
        }
        
        let avg_cpu = if count > 0 { cpu_sum / count as f64 } else { 0.0 };
        let avg_mem = if count > 0 { mem_sum / count as u64 } else { 0 };
        
        Ok((avg_cpu, avg_mem))
    }
    
    fn print_comparison(&self, test_name: &str) {
        println!("\n  === {} Results ===", test_name);
        
        let nginx = &self.nginx_results;
        let destroyer = &self.destroyer_results;
        
        println!("  Throughput:");
        println!("    Nginx:          {:>10.2} req/s", nginx.requests_per_sec);
        println!("    Nginx Destroyer:{:>10.2} req/s", destroyer.requests_per_sec);
        println!("    Improvement:    {:>10.2}x", destroyer.requests_per_sec / nginx.requests_per_sec);
        
        println!("\n  Latency (μs):");
        println!("    Metric      Nginx    Destroyer   Improvement");
        println!("    ------      -----    ---------   -----------");
        println!("    Min         {:>6}   {:>9}   {:>6.2}x", 
                nginx.latency_min, destroyer.latency_min, 
                nginx.latency_min as f64 / destroyer.latency_min as f64);
        println!("    Mean        {:>6.0}   {:>9.0}   {:>6.2}x", 
                nginx.latency_mean, destroyer.latency_mean, 
                nginx.latency_mean / destroyer.latency_mean);
        println!("    P50         {:>6}   {:>9}   {:>6.2}x", 
                nginx.latency_p50, destroyer.latency_p50, 
                nginx.latency_p50 as f64 / destroyer.latency_p50 as f64);
        println!("    P99         {:>6}   {:>9}   {:>6.2}x", 
                nginx.latency_p99, destroyer.latency_p99, 
                nginx.latency_p99 as f64 / destroyer.latency_p99 as f64);
        
        println!("\n  Resource Usage:");
        println!("    CPU Usage:");
        println!("      Nginx:          {:.1}%", nginx.cpu_usage);
        println!("      Nginx Destroyer:{:.1}%", destroyer.cpu_usage);
        println!("    Memory Usage:");
        println!("      Nginx:          {} MB", nginx.memory_usage / 1024);
        println!("      Nginx Destroyer:{} MB", destroyer.memory_usage / 1024);
    }
    
    fn generate_report(&self) {
        println!("\n\n=== FINAL PERFORMANCE REPORT ===");
        println!("\nNginx Destroyer Performance Summary:");
        println!("-----------------------------------");
        
        // Calculate overall performance improvement
        let throughput_improvement = self.destroyer_results.requests_per_sec / self.nginx_results.requests_per_sec;
        let latency_improvement = self.nginx_results.latency_mean / self.destroyer_results.latency_mean;
        
        println!("Overall Performance Improvement: {:.1}x", (throughput_improvement + latency_improvement) / 2.0);
        println!("Throughput Improvement: {:.1}x", throughput_improvement);
        println!("Latency Improvement: {:.1}x", latency_improvement);
        
        println!("\nKey Achievements:");
        println!("- Requests/sec: {:.0} (vs Nginx: {:.0})", 
                self.destroyer_results.requests_per_sec, 
                self.nginx_results.requests_per_sec);
        println!("- Mean latency: {:.0}μs (vs Nginx: {:.0}μs)", 
                self.destroyer_results.latency_mean, 
                self.nginx_results.latency_mean);
        println!("- P99 latency: {}μs (vs Nginx: {}μs)", 
                self.destroyer_results.latency_p99, 
                self.nginx_results.latency_p99);
        
        println!("\nOptimization Techniques Used:");
        println!("- Kernel bypass networking (DPDK)");
        println!("- Zero-copy I/O operations");
        println!("- Lock-free data structures");
        println!("- Memory-mapped file serving");
        println!("- Optimized HTTP parsing");
        println!("- Connection pooling");
        println!("- CPU affinity and NUMA awareness");
        
        if throughput_improvement >= 10.0 {
            println!("\n🎉 GOAL ACHIEVED: 10x+ performance improvement over Nginx!");
        } else {
            println!("\nCurrent improvement: {:.1}x - Keep optimizing!", throughput_improvement);
        }
    }
}

// Simple HTTP client for benchmarking
struct HttpClient {
    socket: net::TcpStream,
    buffer: Vec<u8>,
}

impl HttpClient {
    fn new() -> Result<Self, Error> {
        Ok(HttpClient {
            socket: net::TcpStream::new()?,
            buffer: Vec::with_capacity(65536),
        })
    }
    
    fn connect(host: &str, port: u16) -> Result<Self, Error> {
        let socket = net::TcpStream::connect(&format!("{}:{}", host, port))?;
        socket.set_nodelay(true)?;
        
        Ok(HttpClient {
            socket,
            buffer: Vec::with_capacity(65536),
        })
    }
    
    fn send_request(&mut self, method: &str, path: &str, body: Option<&[u8]>) -> Result<(), Error> {
        self.buffer.clear();
        
        // Build request
        self.buffer.extend_from_slice(method.as_bytes());
        self.buffer.extend_from_slice(b" ");
        self.buffer.extend_from_slice(path.as_bytes());
        self.buffer.extend_from_slice(b" HTTP/1.1\r\n");
        self.buffer.extend_from_slice(b"Host: localhost\r\n");
        
        if let Some(body) = body {
            self.buffer.extend_from_slice(b"Content-Length: ");
            self.buffer.extend_from_slice(body.len().to_string().as_bytes());
            self.buffer.extend_from_slice(b"\r\n");
        }
        
        self.buffer.extend_from_slice(b"\r\n");
        
        if let Some(body) = body {
            self.buffer.extend_from_slice(body);
        }
        
        self.socket.write_all(&self.buffer)?;
        Ok(())
    }
    
    fn read_response(&mut self) -> Result<Vec<u8>, Error> {
        self.buffer.clear();
        let mut total_read = 0;
        let mut headers_complete = false;
        let mut content_length = 0;
        let mut body_start = 0;
        
        loop {
            let mut temp_buf = [0u8; 8192];
            let n = self.socket.read(&mut temp_buf)?;
            if n == 0 {
                break;
            }
            
            self.buffer.extend_from_slice(&temp_buf[..n]);
            total_read += n;
            
            // Parse headers if not done
            if !headers_complete {
                if let Some(pos) = self.find_headers_end() {
                    headers_complete = true;
                    body_start = pos + 4;
                    
                    // Extract content-length
                    let headers = String::from_utf8_lossy(&self.buffer[..pos]);
                    for line in headers.lines() {
                        if line.starts_with("Content-Length:") {
                            if let Ok(len) = line[15..].trim().parse::<usize>() {
                                content_length = len;
                            }
                        }
                    }
                }
            }
            
            // Check if we have the full response
            if headers_complete && self.buffer.len() >= body_start + content_length {
                break;
            }
        }
        
        Ok(self.buffer.clone())
    }
    
    fn find_headers_end(&self) -> Option<usize> {
        let pattern = b"\r\n\r\n";
        self.buffer.windows(pattern.len())
            .position(|window| window == pattern)
    }
}

// Utility functions
fn format_bytes(bytes: usize) -> String {
    if bytes < 1024 {
        format!("{} B", bytes)
    } else if bytes < 1024 * 1024 {
        format!("{:.1} KB", bytes as f64 / 1024.0)
    } else if bytes < 1024 * 1024 * 1024 {
        format!("{:.1} MB", bytes as f64 / (1024.0 * 1024.0))
    } else {
        format!("{:.1} GB", bytes as f64 / (1024.0 * 1024.0 * 1024.0))
    }
}

// Main benchmark entry point
fn main() -> Result<(), Error> {
    println!("Nginx Destroyer Benchmark Suite v1.0");
    println!("====================================\n");
    
    // Check if servers are running
    println!("Checking server availability...");
    if !is_server_running(8080) {
        println!("ERROR: Nginx not running on port 8080");
        return Err(Error.ServerNotRunning);
    }
    if !is_server_running(8081) {
        println!("ERROR: Nginx Destroyer not running on port 8081");
        return Err(Error.ServerNotRunning);
    }
    
    let config = BenchmarkConfig {
        nginx_port: 8080,
        destroyer_port: 8081,
        duration: 30,
        connections: 100,
        threads: num_cpus::get() as u32,
        requests_per_conn: 1000,
        keepalive: true,
        pipelining: 0,
        request_size: 0,
        file_sizes: vec![1024, 10240, 102400, 1048576],
    };
    
    let mut benchmark = NginxComparison::new(config);
    benchmark.run()?;
    
    Ok(())
}

fn is_server_running(port: u16) -> bool {
    net::TcpStream::connect(&format!("127.0.0.1:{}", port)).is_ok()
}