// ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
// ‚ïë üõ°Ô∏è  ‚ïë ‚öñÔ∏è  ‚ïë ‚ö°  ‚ïë
// ‚ïë  C  ‚ïë  E  ‚ïë  G  ‚ïë
// ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
// ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
// ‚ïë wcet [T‚àû] bound ‚ïë
// ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
//
// Author: Ignacio Pe√±a Sep√∫lveda
// Date: June 25, 2025

// ===========================================================================
// TEMPO COMPILER STAGE 2 - LEXER
// ===========================================================================
// Lexer completo con todas las caracter√≠sticas del lenguaje
// ===========================================================================

import "types.tempo"
import "errors.tempo"

// Token types completos
enum TokenType {
    // End of file
    EOF,
    
    // Literals
    NUMBER,
    STRING,
    CHAR,
    TRUE,
    FALSE,
    
    // Identifiers and keywords
    IDENTIFIER,
    
    // Keywords
    FUNCTION,
    TYPE,
    ENUM,
    TRAIT,
    IMPL,
    LET,
    MUT,
    IF,
    ELSE,
    WHILE,
    FOR,
    LOOP,
    MATCH,
    RETURN,
    BREAK,
    CONTINUE,
    DEFER,
    SPAWN,
    ASYNC,
    AWAIT,
    IMPORT,
    EXPORT,
    PUBLIC,
    PRIVATE,
    STATIC,
    CONST,
    WITHIN,
    MEMORY,
    CHANNEL,
    
    // Symbols
    LPAREN,
    RPAREN,
    LBRACE,
    RBRACE,
    LBRACKET,
    RBRACKET,
    SEMICOLON,
    COMMA,
    DOT,
    COLON,
    DOUBLE_COLON,
    ARROW,
    FAT_ARROW,
    EQUALS,
    PLUS_EQUALS,
    MINUS_EQUALS,
    STAR_EQUALS,
    SLASH_EQUALS,
    PERCENT_EQUALS,
    AMPERSAND,
    PIPE,
    CARET,
    QUESTION,
    AT,
    HASH,
    DOLLAR,
    
    // Operators
    PLUS,
    MINUS,
    STAR,
    SLASH,
    PERCENT,
    DOUBLE_AMPERSAND,
    DOUBLE_PIPE,
    EXCLAMATION,
    TILDE,
    DOUBLE_EQUALS,
    NOT_EQUALS,
    LESS_THAN,
    GREATER_THAN,
    LESS_EQUALS,
    GREATER_EQUALS,
    LEFT_SHIFT,
    RIGHT_SHIFT,
    TRIPLE_DOT,
    
    // Time units
    CYCLES,
    NANOSECONDS,
    MICROSECONDS,
    MILLISECONDS,
    SECONDS,
    
    // Memory units
    BYTES,
    KILOBYTES,
    MEGABYTES,
    GIGABYTES,
    
    // Frequency units
    HZ,
    KHZ,
    MHZ,
    GHZ,
}

type Token = {
    type: TokenType,
    value: string,
    line: u32,
    column: u32,
    span: Span,
}

type Span = {
    start: u32,
    end: u32,
    file_id: u32,
}

type Lexer = {
    source: string,
    position: u32,
    line: u32,
    column: u32,
    file_id: u32,
    keywords: Map<string, TokenType>,
    errors: [Error],
}

function new_lexer(source: string, file_id: u32) -> Lexer {
    let keywords = Map<string, TokenType>::new();
    
    // Populate keywords
    keywords.insert("function", TokenType::FUNCTION);
    keywords.insert("type", TokenType::TYPE);
    keywords.insert("enum", TokenType::ENUM);
    keywords.insert("trait", TokenType::TRAIT);
    keywords.insert("impl", TokenType::IMPL);
    keywords.insert("let", TokenType::LET);
    keywords.insert("mut", TokenType::MUT);
    keywords.insert("if", TokenType::IF);
    keywords.insert("else", TokenType::ELSE);
    keywords.insert("while", TokenType::WHILE);
    keywords.insert("for", TokenType::FOR);
    keywords.insert("loop", TokenType::LOOP);
    keywords.insert("match", TokenType::MATCH);
    keywords.insert("return", TokenType::RETURN);
    keywords.insert("break", TokenType::BREAK);
    keywords.insert("continue", TokenType::CONTINUE);
    keywords.insert("defer", TokenType::DEFER);
    keywords.insert("spawn", TokenType::SPAWN);
    keywords.insert("async", TokenType::ASYNC);
    keywords.insert("await", TokenType::AWAIT);
    keywords.insert("import", TokenType::IMPORT);
    keywords.insert("export", TokenType::EXPORT);
    keywords.insert("public", TokenType::PUBLIC);
    keywords.insert("private", TokenType::PRIVATE);
    keywords.insert("static", TokenType::STATIC);
    keywords.insert("const", TokenType::CONST);
    keywords.insert("within", TokenType::WITHIN);
    keywords.insert("memory", TokenType::MEMORY);
    keywords.insert("channel", TokenType::CHANNEL);
    keywords.insert("true", TokenType::TRUE);
    keywords.insert("false", TokenType::FALSE);
    
    // Time units
    keywords.insert("cycles", TokenType::CYCLES);
    keywords.insert("ns", TokenType::NANOSECONDS);
    keywords.insert("¬µs", TokenType::MICROSECONDS);
    keywords.insert("ms", TokenType::MILLISECONDS);
    keywords.insert("s", TokenType::SECONDS);
    
    // Memory units
    keywords.insert("bytes", TokenType::BYTES);
    keywords.insert("KB", TokenType::KILOBYTES);
    keywords.insert("MB", TokenType::MEGABYTES);
    keywords.insert("GB", TokenType::GIGABYTES);
    
    // Frequency units
    keywords.insert("Hz", TokenType::HZ);
    keywords.insert("KHz", TokenType::KHZ);
    keywords.insert("MHz", TokenType::MHZ);
    keywords.insert("GHz", TokenType::GHZ);
    
    return Lexer{
        source,
        position: 0,
        line: 1,
        column: 1,
        file_id,
        keywords,
        errors: [],
    };
}

function next_token(lexer: &mut Lexer) -> Token {
    skip_whitespace_and_comments(lexer);
    
    if is_at_end(lexer) {
        return make_token(lexer, TokenType::EOF, "");
    }
    
    let start_pos = lexer.position;
    let start_line = lexer.line;
    let start_column = lexer.column;
    
    let ch = advance(lexer);
    
    // Numbers
    if is_digit(ch) {
        return scan_number(lexer, start_pos, start_line, start_column);
    }
    
    // Identifiers and keywords
    if is_alpha(ch) || ch == '_' {
        return scan_identifier(lexer, start_pos, start_line, start_column);
    }
    
    // Strings
    if ch == '"' {
        return scan_string(lexer, start_pos, start_line, start_column);
    }
    
    // Characters
    if ch == '\'' {
        return scan_char(lexer, start_pos, start_line, start_column);
    }
    
    // Single and multi-character operators
    match ch {
        '(' => make_token_at(lexer, TokenType::LPAREN, "(", start_pos, start_line, start_column),
        ')' => make_token_at(lexer, TokenType::RPAREN, ")", start_pos, start_line, start_column),
        '{' => make_token_at(lexer, TokenType::LBRACE, "{", start_pos, start_line, start_column),
        '}' => make_token_at(lexer, TokenType::RBRACE, "}", start_pos, start_line, start_column),
        '[' => make_token_at(lexer, TokenType::LBRACKET, "[", start_pos, start_line, start_column),
        ']' => make_token_at(lexer, TokenType::RBRACKET, "]", start_pos, start_line, start_column),
        ';' => make_token_at(lexer, TokenType::SEMICOLON, ";", start_pos, start_line, start_column),
        ',' => make_token_at(lexer, TokenType::COMMA, ",", start_pos, start_line, start_column),
        '@' => make_token_at(lexer, TokenType::AT, "@", start_pos, start_line, start_column),
        '#' => make_token_at(lexer, TokenType::HASH, "#", start_pos, start_line, start_column),
        '$' => make_token_at(lexer, TokenType::DOLLAR, "$", start_pos, start_line, start_column),
        '?' => make_token_at(lexer, TokenType::QUESTION, "?", start_pos, start_line, start_column),
        '~' => make_token_at(lexer, TokenType::TILDE, "~", start_pos, start_line, start_column),
        
        '.' => {
            if peek(lexer) == '.' && peek_next(lexer) == '.' {
                advance(lexer);
                advance(lexer);
                make_token_at(lexer, TokenType::TRIPLE_DOT, "...", start_pos, start_line, start_column)
            } else {
                make_token_at(lexer, TokenType::DOT, ".", start_pos, start_line, start_column)
            }
        }
        
        ':' => {
            if peek(lexer) == ':' {
                advance(lexer);
                make_token_at(lexer, TokenType::DOUBLE_COLON, "::", start_pos, start_line, start_column)
            } else {
                make_token_at(lexer, TokenType::COLON, ":", start_pos, start_line, start_column)
            }
        }
        
        '=' => {
            if peek(lexer) == '=' {
                advance(lexer);
                make_token_at(lexer, TokenType::DOUBLE_EQUALS, "==", start_pos, start_line, start_column)
            } else if peek(lexer) == '>' {
                advance(lexer);
                make_token_at(lexer, TokenType::FAT_ARROW, "=>", start_pos, start_line, start_column)
            } else {
                make_token_at(lexer, TokenType::EQUALS, "=", start_pos, start_line, start_column)
            }
        }
        
        '+' => {
            if peek(lexer) == '=' {
                advance(lexer);
                make_token_at(lexer, TokenType::PLUS_EQUALS, "+=", start_pos, start_line, start_column)
            } else {
                make_token_at(lexer, TokenType::PLUS, "+", start_pos, start_line, start_column)
            }
        }
        
        '-' => {
            if peek(lexer) == '>' {
                advance(lexer);
                make_token_at(lexer, TokenType::ARROW, "->", start_pos, start_line, start_column)
            } else if peek(lexer) == '=' {
                advance(lexer);
                make_token_at(lexer, TokenType::MINUS_EQUALS, "-=", start_pos, start_line, start_column)
            } else {
                make_token_at(lexer, TokenType::MINUS, "-", start_pos, start_line, start_column)
            }
        }
        
        '*' => {
            if peek(lexer) == '=' {
                advance(lexer);
                make_token_at(lexer, TokenType::STAR_EQUALS, "*=", start_pos, start_line, start_column)
            } else {
                make_token_at(lexer, TokenType::STAR, "*", start_pos, start_line, start_column)
            }
        }
        
        '/' => {
            if peek(lexer) == '=' {
                advance(lexer);
                make_token_at(lexer, TokenType::SLASH_EQUALS, "/=", start_pos, start_line, start_column)
            } else {
                make_token_at(lexer, TokenType::SLASH, "/", start_pos, start_line, start_column)
            }
        }
        
        '%' => {
            if peek(lexer) == '=' {
                advance(lexer);
                make_token_at(lexer, TokenType::PERCENT_EQUALS, "%=", start_pos, start_line, start_column)
            } else {
                make_token_at(lexer, TokenType::PERCENT, "%", start_pos, start_line, start_column)
            }
        }
        
        '!' => {
            if peek(lexer) == '=' {
                advance(lexer);
                make_token_at(lexer, TokenType::NOT_EQUALS, "!=", start_pos, start_line, start_column)
            } else {
                make_token_at(lexer, TokenType::EXCLAMATION, "!", start_pos, start_line, start_column)
            }
        }
        
        '<' => {
            if peek(lexer) == '=' {
                advance(lexer);
                make_token_at(lexer, TokenType::LESS_EQUALS, "<=", start_pos, start_line, start_column)
            } else if peek(lexer) == '<' {
                advance(lexer);
                make_token_at(lexer, TokenType::LEFT_SHIFT, "<<", start_pos, start_line, start_column)
            } else {
                make_token_at(lexer, TokenType::LESS_THAN, "<", start_pos, start_line, start_column)
            }
        }
        
        '>' => {
            if peek(lexer) == '=' {
                advance(lexer);
                make_token_at(lexer, TokenType::GREATER_EQUALS, ">=", start_pos, start_line, start_column)
            } else if peek(lexer) == '>' {
                advance(lexer);
                make_token_at(lexer, TokenType::RIGHT_SHIFT, ">>", start_pos, start_line, start_column)
            } else {
                make_token_at(lexer, TokenType::GREATER_THAN, ">", start_pos, start_line, start_column)
            }
        }
        
        '&' => {
            if peek(lexer) == '&' {
                advance(lexer);
                make_token_at(lexer, TokenType::DOUBLE_AMPERSAND, "&&", start_pos, start_line, start_column)
            } else {
                make_token_at(lexer, TokenType::AMPERSAND, "&", start_pos, start_line, start_column)
            }
        }
        
        '|' => {
            if peek(lexer) == '|' {
                advance(lexer);
                make_token_at(lexer, TokenType::DOUBLE_PIPE, "||", start_pos, start_line, start_column)
            } else {
                make_token_at(lexer, TokenType::PIPE, "|", start_pos, start_line, start_column)
            }
        }
        
        '^' => make_token_at(lexer, TokenType::CARET, "^", start_pos, start_line, start_column),
        
        _ => {
            let error_msg = format!("Unexpected character: '{}'", ch);
            add_error(lexer, error_msg, start_line, start_column);
            make_token_at(lexer, TokenType::EOF, "", start_pos, start_line, start_column)
        }
    }
}

function scan_number(lexer: &mut Lexer, start_pos: u32, start_line: u32, start_column: u32) -> Token {
    // Scan integer part
    while is_digit(peek(lexer)) {
        advance(lexer);
    }
    
    // Check for decimal part
    if peek(lexer) == '.' && is_digit(peek_next(lexer)) {
        advance(lexer); // consume '.'
        while is_digit(peek(lexer)) {
            advance(lexer);
        }
    }
    
    // Check for exponent
    if peek(lexer) == 'e' || peek(lexer) == 'E' {
        advance(lexer);
        if peek(lexer) == '+' || peek(lexer) == '-' {
            advance(lexer);
        }
        while is_digit(peek(lexer)) {
            advance(lexer);
        }
    }
    
    // Check for type suffix (i32, i64, f32, f64, etc.)
    if peek(lexer) == 'i' || peek(lexer) == 'u' || peek(lexer) == 'f' {
        let type_start = lexer.position;
        advance(lexer);
        while is_digit(peek(lexer)) {
            advance(lexer);
        }
    }
    
    let value = substring(lexer.source, start_pos, lexer.position);
    make_token_at(lexer, TokenType::NUMBER, value, start_pos, start_line, start_column)
}

function scan_identifier(lexer: &mut Lexer, start_pos: u32, start_line: u32, start_column: u32) -> Token {
    while is_alphanumeric(peek(lexer)) || peek(lexer) == '_' {
        advance(lexer);
    }
    
    let value = substring(lexer.source, start_pos, lexer.position);
    
    // Check if it's a keyword
    let token_type = match lexer.keywords.get(&value) {
        Some(type) => *type,
        None => TokenType::IDENTIFIER,
    };
    
    make_token_at(lexer, token_type, value, start_pos, start_line, start_column)
}

function scan_string(lexer: &mut Lexer, start_pos: u32, start_line: u32, start_column: u32) -> Token {
    let mut value = String::new();
    
    while peek(lexer) != '"' && !is_at_end(lexer) {
        if peek(lexer) == '\n' {
            lexer.line += 1;
            lexer.column = 1;
        }
        
        if peek(lexer) == '\\' {
            advance(lexer);
            match peek(lexer) {
                'n' => { value.push('\n'); advance(lexer); }
                'r' => { value.push('\r'); advance(lexer); }
                't' => { value.push('\t'); advance(lexer); }
                '\\' => { value.push('\\'); advance(lexer); }
                '"' => { value.push('"'); advance(lexer); }
                '$' => { value.push('$'); advance(lexer); }
                _ => {
                    add_error(lexer, "Invalid escape sequence", lexer.line, lexer.column);
                    advance(lexer);
                }
            }
        } else if peek(lexer) == '$' && peek_next(lexer) == '{' {
            // String interpolation
            // TODO: Handle interpolation properly
            advance(lexer);
        } else {
            value.push(advance(lexer));
        }
    }
    
    if is_at_end(lexer) {
        add_error(lexer, "Unterminated string", start_line, start_column);
    } else {
        advance(lexer); // Consume closing quote
    }
    
    make_token_at(lexer, TokenType::STRING, value, start_pos, start_line, start_column)
}

function scan_char(lexer: &mut Lexer, start_pos: u32, start_line: u32, start_column: u32) -> Token {
    let ch = if peek(lexer) == '\\' {
        advance(lexer);
        match advance(lexer) {
            'n' => '\n',
            'r' => '\r',
            't' => '\t',
            '\\' => '\\',
            '\'' => '\'',
            _ => {
                add_error(lexer, "Invalid escape sequence in char literal", lexer.line, lexer.column);
                '\0'
            }
        }
    } else {
        advance(lexer)
    };
    
    if peek(lexer) != '\'' {
        add_error(lexer, "Unterminated char literal", start_line, start_column);
    } else {
        advance(lexer); // Consume closing quote
    }
    
    make_token_at(lexer, TokenType::CHAR, ch.to_string(), start_pos, start_line, start_column)
}

function skip_whitespace_and_comments(lexer: &mut Lexer) {
    loop {
        match peek(lexer) {
            ' ' | '\r' | '\t' => {
                advance(lexer);
            }
            '\n' => {
                lexer.line += 1;
                lexer.column = 1;
                lexer.position += 1;
            }
            '/' => {
                if peek_next(lexer) == '/' {
                    // Line comment
                    advance(lexer);
                    advance(lexer);
                    while peek(lexer) != '\n' && !is_at_end(lexer) {
                        advance(lexer);
                    }
                } else if peek_next(lexer) == '*' {
                    // Block comment
                    advance(lexer);
                    advance(lexer);
                    while !(peek(lexer) == '*' && peek_next(lexer) == '/') && !is_at_end(lexer) {
                        if peek(lexer) == '\n' {
                            lexer.line += 1;
                            lexer.column = 1;
                        }
                        advance(lexer);
                    }
                    if !is_at_end(lexer) {
                        advance(lexer); // *
                        advance(lexer); // /
                    }
                } else {
                    return;
                }
            }
            _ => return,
        }
    }
}

// Helper functions
function is_at_end(lexer: &Lexer) -> bool {
    lexer.position >= lexer.source.len()
}

function advance(lexer: &mut Lexer) -> char {
    let ch = lexer.source.chars().nth(lexer.position).unwrap_or('\0');
    lexer.position += 1;
    lexer.column += 1;
    ch
}

function peek(lexer: &Lexer) -> char {
    lexer.source.chars().nth(lexer.position).unwrap_or('\0')
}

function peek_next(lexer: &Lexer) -> char {
    lexer.source.chars().nth(lexer.position + 1).unwrap_or('\0')
}

function is_digit(ch: char) -> bool {
    ch >= '0' && ch <= '9'
}

function is_alpha(ch: char) -> bool {
    (ch >= 'a' && ch <= 'z') || (ch >= 'A' && ch <= 'Z')
}

function is_alphanumeric(ch: char) -> bool {
    is_alpha(ch) || is_digit(ch)
}

function make_token(lexer: &Lexer, type: TokenType, value: string) -> Token {
    Token{
        type,
        value,
        line: lexer.line,
        column: lexer.column,
        span: Span{
            start: lexer.position,
            end: lexer.position,
            file_id: lexer.file_id,
        },
    }
}

function make_token_at(
    lexer: &Lexer, 
    type: TokenType, 
    value: string, 
    start_pos: u32, 
    line: u32, 
    column: u32
) -> Token {
    Token{
        type,
        value,
        line,
        column,
        span: Span{
            start: start_pos,
            end: lexer.position,
            file_id: lexer.file_id,
        },
    }
}

function add_error(lexer: &mut Lexer, message: string, line: u32, column: u32) {
    lexer.errors.push(Error{
        message,
        line,
        column,
        file_id: lexer.file_id,
    });
}

function substring(s: string, start: u32, end: u32) -> string {
    s[start..end].to_string()
}